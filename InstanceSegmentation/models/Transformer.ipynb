{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=256, num_hidden=512, num_heads=8, dropout=0.1, bias=False, Masked=False, **kwargs):\n",
    "        \"\"\"\n",
    "        :param d_model: 输入的特征维度\n",
    "        :param num_hidden: 线性变换的隐藏层维度\n",
    "        :param num_heads: 注意力头的数量\n",
    "        :param dropout: dropout概率\n",
    "        :param bias: 是否使用偏置\n",
    "        :param Masked: 是否使用掩码\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.Masked = Masked\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.WQ = nn.Linear(d_model, num_hidden, bias=bias) # 线性变换，将输入转换为num_hidden维度\n",
    "        self.WK = nn.Linear(d_model, num_hidden, bias=bias) # shape: [batch_size, seq_len, num_hidden]\n",
    "        self.WV = nn.Linear(d_model, num_hidden, bias=bias)\n",
    "        self.WO = nn.Linear(num_hidden, d_model, bias=bias) # shape: [batch_size, seq_len, d_model]\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, seq_len, d_model]\n",
    "        :return: [batch_size, num_heads, seq_len, heads_features]\n",
    "        \"\"\"\n",
    "        x = x.view(x.size(0), x.size(1), self.num_heads, -1) # 将d_model拆分为num_heads × feature\n",
    "        return x.permute(0, 2, 1, 3) # 将num_heads提前到第二维，相当于将输入拆分为num_heads个子输入\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch_size, num_heads, seq_len, heads_features]\n",
    "        :return: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 2, 1, 3) # 将num_heads提前到第三维\n",
    "        return x.contiguous().view(x.size(0), x.size(1), -1) # 将num_heads × feature合并为d_model，便于多种操作\n",
    "    def get_padding_mask(self, X, valid_lens, value=-1e9):\n",
    "        \"\"\"\n",
    "        :param X: [batch_size, num_heads, seq_len, seq_len]\n",
    "        :param valid_lens: [batch_size]\n",
    "        :param value: 填充值\n",
    "        :return: [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        if valid_lens is None:\n",
    "            return F.softmax(X, dim=-1)\n",
    "        else:\n",
    "            shape = X.shape\n",
    "            if valid_lens.dim() == 1:\n",
    "                mask = torch.arange(shape[-1])[None, :] >= valid_lens[:, None] # 生成填充掩码\n",
    "            else:\n",
    "                raise ValueError(\"valid_lens must be 1D\")\n",
    "            padding_mask = mask.unsqueeze(1).unsqueeze(1).repeat(1, shape[1], shape[2], 1) \n",
    "            return padding_mask\n",
    "    def scaled_dot_product_attention(self, Q_head, K_head, V_head, valid_lens):\n",
    "        \"\"\"\n",
    "        :param Q_head: [batch_size, num_heads, query_seq_len, heads_features]\n",
    "        :param K_head: [batch_size, num_heads, key_seq_len, heads_features]\n",
    "        :param V_head: [batch_size, num_heads, value_seq_len, heads_features] -> key_seq_len == value_seq_len\n",
    "        :param valid_lens: [batch_size] 每个样本的有效长度\n",
    "        :return: [batch_size, num_heads, value_seq_len, heads_features]\n",
    "        \"\"\"\n",
    "        d_k = Q_head.size(-1)\n",
    "        query_seq_len = Q_head.size(-2)\n",
    "        key_seq_len = K_head.size(-2)\n",
    "        scores = torch.matmul(Q_head, K_head.permute(0, 1, 3, 2)) / np.sqrt(d_k) # Q与K的点积\n",
    "        padding_mask = self.get_padding_mask(scores, valid_lens) # 填充掩码，填充值为负无穷\n",
    "        if self.Masked == True:\n",
    "            causal_mask = torch.tril(torch.ones(query_seq_len, key_seq_len)).bool() # 因果掩码，只保留左下角的元素，防止获取未来信息\n",
    "            mask = padding_mask & causal_mask # 与操作，只保留填充值和因果掩码的交集\n",
    "        else:\n",
    "            mask = padding_mask\n",
    "        masked_scores = scores.masked_fill(mask, -1e9) # 忽略填充值和未来信息\n",
    "        weights = F.softmax(self.dropout(masked_scores), dim=-1) # 经过dropout和softmax\n",
    "        attention = torch.matmul(weights, V_head) # 加权求和\n",
    "        return attention\n",
    "    def forward(self, Q, K, V, valid_lens):\n",
    "        \"\"\"\n",
    "        :param Q: [batch_size, query_seq_len, d_model]\n",
    "        :param K: [batch_size, key_seq_len, d_model]\n",
    "        :param V: [batch_size, value_seq_len, d_model] -> key_seq_len == value_seq_len\n",
    "        :param valid_lens: [batch_size]\n",
    "        :return: [batch_size, value_seq_len, d_model]\n",
    "        \"\"\"\n",
    "        Q_head = self.split_heads(self.WQ(Q)) # 拆分Q，K，V为num_heads个子输入\n",
    "        K_head = self.split_heads(self.WK(K)) # shape: [batch_size, num_heads, seq_len, feature]\n",
    "        V_head = self.split_heads(self.WV(V)) \n",
    "        attention = self.scaled_dot_product_attention(Q_head, K_head, V_head, valid_lens)\n",
    "        attention = self.combine_heads(attention)\n",
    "        return self.WO(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: torch.Size([8, 8, 5, 5])\n",
      "weights: torch.Size([8, 8, 5, 5])\n",
      "torch.Size([8, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.rand(8, 5, 256) * 10\n",
    "K = torch.rand(8, 5, 256) * 10\n",
    "V = torch.rand(8, 5, 256) * 10\n",
    "model = MultiHeadAttention(d_model=256, num_hidden=512, num_heads=8)\n",
    "value_lens = torch.tensor([1, 2, 3, 4, 3, 2, 3, 4])\n",
    "output = model(Q, K, V, value_lens)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm 对输入的每个channel进行归一化\n",
    "    LayerNorm 对输入的每个样本进行归一化\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True)\n",
    "        x = (x - mean) / torch.sqrt(var +self.eps)\n",
    "        out = self.gamma * x + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden, output_size, dropout=0.1, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, num_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(num_hidden, output_size)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(normalized_shape)\n",
    "    def forward(self, X, Y):\n",
    "        # X: 输入，Y: 经过多头注意力或者FFN的输出\n",
    "        return self.norm(self.dropout(X + Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=1000):\n",
    "        super(PositionEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P = torch.zeros((1, max_len, d_model))\n",
    "        X = torch.arange(max_len).reshape(-1, 1) / torch.pow(10000, torch.arange(0, d_model, 2) / d_model)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        # X: [batch_size, seq_len, d_model]\n",
    "        X = X + self.P[:, :X.size(1), :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn=1, dropout=0, **kwargs):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_hidden, num_heads, dropout, bias=False, Masked=False)\n",
    "        self.addnorm1 = AddNorm(d_model, dropout)\n",
    "        self.ffn = nn.Sequential()\n",
    "        for i in range(num_ffn):\n",
    "            self.ffn.add_module(f'ffn_{i}', PositionWiseFFN(d_model, ffn_num_hidden, d_model, dropout))\n",
    "        self.addnorm2 = AddNorm(d_model, dropout)\n",
    "    def forward(self, X, valid_lens):\n",
    "        output = self.attention(X, X, X, valid_lens)\n",
    "        output = self.addnorm1(X, output)\n",
    "        X = output\n",
    "        for ffn in self.ffn:\n",
    "            output = ffn(output)\n",
    "        output = self.addnorm2(X, output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn=1, num_layers=1, dropout=0, **kwargs):\n",
    "        \"\"\"\n",
    "        :param vocab_size: 词典大小（多少个不同的词）\n",
    "        :param d_model: 要输出的词向量维度\n",
    "        :param num_hidden: 线性变换的隐藏层维度\n",
    "        :param num_heads: 注意力头的数量\n",
    "        :param ffn_num_hidden: FFN的隐藏层维度\n",
    "        :param num_ffn: FFN的层数\n",
    "        :param num_layers: 编码器的层数\n",
    "        :param dropout: dropout概率\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = PositionEncoding(d_model, dropout)\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.encoder.add_module(f'encoder_{i}', EncoderBlock(d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn, dropout))\n",
    "    def forward(self, X, valid_lens):\n",
    "        # 因为位置编码值在 -1~1 之间，所以Embedding编码值需要乘以d_model的平方根，进行缩放，类似于Scaled Dot-Product Attention\n",
    "        X = self.embedding(X) * np.sqrt(self.d_model)\n",
    "        X = self.pos_embedding(X)\n",
    "        self._attention_weights = [None] * len(self.encoder)\n",
    "        for i, encoder in enumerate(self.encoder):\n",
    "            X = encoder(X, valid_lens)\n",
    "            self._attention_weights[i] = encoder.attention.attention.attention_weights\n",
    "        return X\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn=1, dropout=0, num_layers=1, **kwargs):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(d_model, num_hidden, num_heads, dropout, bias=False, Masked=True)\n",
    "        self.addnorm1 = AddNorm(d_model, dropout)\n",
    "        self.attention2 = MultiHeadAttention(d_model, num_hidden, num_heads, dropout, bias=False, Masked=False)\n",
    "        self.addnorm2 = AddNorm(d_model, dropout)\n",
    "        self.ffn = nn.Sequential()\n",
    "        self.num_layers = num_layers\n",
    "        for i in range(num_ffn):\n",
    "            self.ffn.add_module(f'ffn_{i}', PositionWiseFFN(d_model, ffn_num_hidden, d_model, dropout))\n",
    "        self.addnorm3 = AddNorm(d_model, dropout)\n",
    "    def forward(self, X, state):\n",
    "        # 在预测阶段，输入的X是包含了当前时间步的词元以及之前所有时间步词元的序列，需要在每次输出后，将最新的预测值（即X[-1]）与输入进行拼接\n",
    "        # 预测阶段需要重新写代码实现\n",
    "        enc_outputs, enc_valid_lens, dec_valid_lens = state[0], state[1], state[2]\n",
    "        output = self.attention1(X, X, X, dec_valid_lens)\n",
    "        output = self.addnorm1(X, output)\n",
    "        X = output\n",
    "        output = self.attention2(output, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        output = self.addnorm2(X, output)\n",
    "        X = output\n",
    "        for ffn in self.ffn:\n",
    "            output = ffn(output)\n",
    "        output = self.addnorm3(X, output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn=1, dropout=0, num_layers=1, **kwargs):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention1 = MultiHeadAttention(d_model, num_hidden, num_heads, dropout, bias=False, Masked=True)\n",
    "        self.addnorm1 = AddNorm(d_model, dropout)\n",
    "        self.attention2 = MultiHeadAttention(d_model, num_hidden, num_heads, dropout, bias=False, Masked=False)\n",
    "        self.addnorm2 = AddNorm(d_model, dropout)\n",
    "        self.ffn = nn.Sequential()\n",
    "        self.num_layers = num_layers\n",
    "        for i in range(num_ffn):\n",
    "            self.ffn.add_module(f'ffn_{i}', PositionWiseFFN(d_model, ffn_num_hidden, d_model, dropout))\n",
    "        self.addnorm3 = AddNorm(d_model, dropout)\n",
    "    def forward(self, X, state):\n",
    "        \"\"\"\n",
    "        对于掩码自注意力机制：Q，K，V都是X\n",
    "        在训练阶段，输入全部序列，输出也是全部序列\n",
    "        在预测阶段，第一个时间步输入的X是单个词元/起始符，之后时间步包含了当前时间步的词元以及之前所有时间步词元的序列\n",
    "            因此，需要在每次输出后，将最新的预测值（即X[-1]）与输入进行拼接\n",
    "            PS：在预测过程中，每次都会对之前所有的时间步进行预测，但是只有最后一个时间步的预测值（最新预测值）才会与输入拼接，并被用于下一个时间步的预测\n",
    "                每次预测中，之前时间步的预测值只是为了使当前时间步的预测更准确，因为这样可以更好地利用上下文信息\n",
    "        \"\"\"\n",
    "        enc_outputs, enc_valid_lens, dec_valid_lens = state[0], state[1], state[2]\n",
    "        output = self.attention1(X, X, X, dec_valid_lens)\n",
    "        output = self.addnorm1(X, output)\n",
    "        X = output\n",
    "        output = self.attention2(output, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        output = self.addnorm2(X, output)\n",
    "        X = output\n",
    "        for ffn in self.ffn:\n",
    "            output = ffn(output)\n",
    "        output = self.addnorm3(X, output)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn=1, num_layers=1, dropout=0, training=True, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = PositionEncoding(d_model, dropout)\n",
    "        self.decoder = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.decoder.add_module(f'decoder_{i}', DecoderBlock(d_model, num_hidden, num_heads, ffn_num_hidden, num_ffn, dropout, i))\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.training = training\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, dec_valid_lens): # 初始化解码器的状态\n",
    "        if self.training:\n",
    "            # 训练阶段，所有（batch_size个）句子一同输入Decoder，因此state[2]为每个句子的有效长度（句子中词的个数），shape: [batch_size]\n",
    "            return [enc_outputs, enc_valid_lens, dec_valid_lens]\n",
    "        else:\n",
    "            # 预测阶段，每个时间步只解码一个词元，因此state[2]为None\n",
    "            return [enc_outputs, enc_valid_lens, None]\n",
    "    def forward(self, X, state):\n",
    "        \"\"\"\n",
    "        :param X: [batch_size, seq_len]\n",
    "        :param state: enc_outputs, enc_valid_lens, key_values\n",
    "        :return: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        X = self.embedding(X) * np.sqrt(self.d_model)\n",
    "        X = self.pos_embedding(X)\n",
    "        self._attention_weights = [[None] * len(self.decoder) for _ in range(2)]\n",
    "        for i, decoder in enumerate(self.decoder):\n",
    "            X, state = decoder(X, state)\n",
    "            self._attention_weights[0][i] = decoder.attention1.attention.attention_weights # 解码器自注意力权重\n",
    "            self._attention_weights[1][i] = decoder.attention2.attention.attention_weights # 编码器-解码器自注意力权重\n",
    "        return self.fc(X), state\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.start_token = 0 # 起始符索引\n",
    "        self.end_token = 1 # 结束符索引\n",
    "        self.output = None # 输出\n",
    "    def forward(self, enc_X, dec_X, enc_valid_lens, dec_valid_lens):\n",
    "        enc_outputs = self.encoder(enc_X, enc_valid_lens)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, enc_valid_lens, dec_valid_lens)\n",
    "        return self.decoder(dec_X, dec_state)\n",
    "    def predict(self, enc_X, dec_X, enc_valid_lens):\n",
    "        # 预测阶段，每个时间步只解码一个词元，因此state[2]为None\n",
    "        # enc_X: [batch_size, seq_len], dec_X: [batch_size, 1], enc_valid_lens: [batch_size]\n",
    "        # PS：Encoder和Decoder中输入的词元均为词元的索引\n",
    "        enc_outputs = self.encoder(enc_X, enc_valid_lens)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, enc_valid_lens, None)\n",
    "        while self.output != self.end:\n",
    "            self.output = torch.argmax(self.decoder(dec_X, dec_state)[:, -1, :], dim=-1)[:, None] # 输出shape: [batch_size, 1]\n",
    "            dec_X = torch.cat((dec_X, self.output), dim=1)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先将词元转换为词元的索引，然后将词元的索引输入EncoderDecoder\n",
    "# EncoderDecoder的输入是词元的索引，输出也是词元的索引\n",
    "# 词元的索引是词典中词元的位置\n",
    "# 1. 构建词典\n",
    "# 2. 构建函数，将词典中的词元转换为词元的索引，并将词元的索引转换为词典中的词元\n",
    "\n",
    "## 词典介绍：\n",
    "# 词典中的词元是唯一的，每个词元对应一个索引\n",
    "# 词典中的词元的索引是从0开始的，0表示起始符，1表示结束符\n",
    "# 词典中的词元的索引是按照词典中词元的出现频率排序的，出现频率越高，索引越小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
